{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_frequency done reading vacancy table\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/rowan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/rowan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_frequency done get_TF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pymysql\n",
    "pymysql.install_as_MySQLdb()\n",
    "from pandas.io import sql\n",
    "import MySQLdb\n",
    "from pandas.io import sql\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import pymysql\n",
    "\n",
    "# Create a connection object\n",
    "dbServerName    = \"127.0.0.1\"\n",
    "dbUser          = \"root\"\n",
    "dbPassword      = \"password\"\n",
    "dbName          = \"JobPosts\"\n",
    "\n",
    " \n",
    "connectionObject   = pymysql.connect(host=dbServerName, user=dbUser, password=dbPassword,\n",
    "\n",
    "                                     db=dbName)\n",
    "\n",
    "\n",
    "try:\n",
    "    # Create a cursor object\n",
    "\n",
    "    cursorObject = connectionObject.cursor() \n",
    "    selectStatement=\"SELECT Vacancy.Vacancy_id ,Vacancy.City , Job.Job_name ,Vacancy.job_category , Vacancy.job_description ,Vacancy.Job_Date, \\\n",
    "      Vacancy.num_vacancies, Vacancy.salary_maximum , Vacancy.salary_minimum , Vacancy.career_level \\\n",
    "      from Vacancy,Job where Vacancy.Job_id2 = Job.Job_id \"\n",
    "\n",
    "    cursorObject.execute(selectStatement)\n",
    "    table_rows = cursorObject.fetchall()\n",
    "    vacancy = pd.DataFrame(list(table_rows), columns=['Vacancy_id','City','job_title','job_category','job_description','Job_Date'\\\n",
    "                                                     ,'num_vacancies','salary_maximum','salary_minimum','career_level'])\n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "    print(\"Exeception occured:{}\".format(e))\n",
    "\n",
    "finally:\n",
    "\n",
    "    connectionObject.commit()\n",
    "\n",
    "print(\"new_frequency done reading vacancy table\")\n",
    "\n",
    " \n",
    "# connectionObject2   = pymysql.connect(host=dbServerName, user=dbUser, password=dbPassword,\n",
    "\n",
    "#                                      db=dbName)\n",
    "\n",
    "# try:\n",
    "#     # Create a cursor object\n",
    "\n",
    "#     cursorObject2 = connectionObject2.cursor() \n",
    "#     selectStatement2=\"SELECT Job.Job_id,Job.Job_name  from Job \"\n",
    "\n",
    "#     cursorObject2.execute(selectStatement2)\n",
    "#     table_rows2 = cursorObject2.fetchall()\n",
    "#     job = pd.DataFrame(list(table_rows2), columns=['Job_id','Job_name'])\n",
    "\n",
    "# except Exception as e:\n",
    "\n",
    "#     print(\"Exeception occured:{}\".format(e))\n",
    "\n",
    "# finally:\n",
    "\n",
    "#     connectionObject2.close()\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "vacancy[\"job_description\"].fillna( ' ', inplace = True) \n",
    "\n",
    "\"\"\"#TF-IDF\"\"\"\n",
    "\n",
    "#Fetch wordcount for each abstract\n",
    "vacancy['word_count'] = vacancy['job_description'].apply(lambda x: len(str(x).split(\" \")))\n",
    "vacancy[['job_description','word_count']].head()\n",
    "\n",
    "vacancy.word_count.describe()\n",
    "\n",
    "freq = pd.Series(' '.join(vacancy['job_description']).split()).value_counts()[:20]\n",
    "#freq\n",
    "#Identify uncommon words\n",
    "freq1 =  pd.Series(' '.join(vacancy \n",
    "         ['job_description']).split()).value_counts()[-20:]\n",
    "#freq1\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "# Libraries for text preprocessing\n",
    "import re\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#nltk.download('wordnet') \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "stem = PorterStemmer()\n",
    "##Creating a list of stop words and adding custom stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "##Creating a list of custom stopwords\n",
    "new_words = [\"bull\",\"nbsp\",\"amp\"]\n",
    "stop_words = stop_words.union(new_words)\n",
    "\n",
    "def remove_html_tags(text):\n",
    " #   \"\"\"Remove html tags from a string\"\"\"\n",
    "    import re\n",
    "    clean = re.compile('(\\<(\\/)?(\\w)*(\\d)?\\>)')\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "corpus = []\n",
    "vacancy_count = len(vacancy)\n",
    "for i in range(0,vacancy_count ):\n",
    "        #remove tags\n",
    "    text = remove_html_tags(vacancy['job_description'][i])\n",
    "    \n",
    "    #Remove punctuations\n",
    "    text = re.sub('[^a-zA-Z]', ' ',text)\n",
    "    \n",
    "    #Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "\n",
    "    #text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    \n",
    "    ##Convert to list from string\n",
    "    text = text.split()\n",
    "    \n",
    "    ##Stemming\n",
    "    ps=PorterStemmer()\n",
    "    #Lemmatisation\n",
    "    lem = WordNetLemmatizer()\n",
    "    text = [lem.lemmatize(word) for word in text if not word in  \n",
    "            stop_words] \n",
    "    text = \" \".join(text)\n",
    "    corpus.append(text)\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "cv=CountVectorizer(stop_words=stop_words, max_features=10000, ngram_range=(1,3))\n",
    "#df2['job_description'] = df2['job_description'].apply(lambda x: cv.fit_transform(x))\n",
    "\n",
    "X=cv.fit_transform(corpus)\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    " \n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(X)\n",
    "# get feature names\n",
    "feature_names=cv.get_feature_names()\n",
    " \n",
    "# fetch document for which keywords needs to be extracted\n",
    "\n",
    " \n",
    "#generate tf-idf for the given document\n",
    "#tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n",
    "\n",
    "\n",
    "#Function for sorting tf_idf in descending order\n",
    "from scipy.sparse import coo_matrix\n",
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    " \n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    " \n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    " \n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def get_TF(text):\n",
    "  tf_idf_vector=tfidf_transformer.transform(cv.transform([text]))\n",
    "  #sort the tf-idf vectors by descending order of scores\n",
    "  sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "  #extract only the top n; n here is 10\n",
    "  keywords=extract_topn_from_vector(feature_names,sorted_items,20)\n",
    "  keys=[]\n",
    "  for k in keywords:\n",
    "    keys.append(k)\n",
    "  return keys\n",
    "\n",
    "df2=vacancy\n",
    "df2['job_description'] = df2['job_description'].apply(lambda x: get_TF(x))\n",
    "\n",
    "\n",
    "print(\"new_frequency done get_TF\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "pymysql.install_as_MySQLdb()\n",
    "from pandas.io import sql\n",
    "import MySQLdb\n",
    "from pandas.io import sql\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import pymysql\n",
    "\n",
    "# Create a connection object\n",
    "dbServerName    = \"127.0.0.1\"\n",
    "dbUser          = \"root\"\n",
    "dbPassword      = \"password\"\n",
    "dbName          = \"JobPosts\"\n",
    "\n",
    " \n",
    "connectionObject   = pymysql.connect(host=dbServerName, user=dbUser, password=dbPassword,\n",
    "\n",
    "                                     db=dbName)\n",
    "\n",
    "try:\n",
    "    # Create a cursor object\n",
    "\n",
    "    cursorObject = connectionObject.cursor() \n",
    "    selectStatement=\"SELECT * FROM has_viewed\"\n",
    "    cursorObject.execute(selectStatement)\n",
    "    table_rows = cursorObject.fetchall()\n",
    "    has_viewed = pd.DataFrame(list(table_rows), columns=['Vacancy_id3','User_id3'])\n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "    print(\"Exeception occured:{}\".format(e))\n",
    "\n",
    "finally:\n",
    "\n",
    "    connectionObject.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#from rake_nltk import Rake\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df=vacancy\n",
    "\n",
    "def convert(list): \n",
    "      \n",
    "    # Converting integer list to string list \n",
    "    s = [str(i) for i in list] \n",
    "      \n",
    "    # Join list items using join() \n",
    "    res =\" \".join(s)\n",
    "      \n",
    "    return(res)\n",
    "vacancy['job_description'] = vacancy['job_description'].apply(lambda x: convert(x))\n",
    "\n",
    "\n",
    "from pandas import DataFrame \n",
    "\n",
    "temp=vacancy['City'].map(str) + ' ' + df['job_title'].map(str)+ ' ' + df['job_category'].map(str) + ' ' + df['salary_minimum'].map(str) + ' ' + df['salary_maximum'].map(str)+ ' ' + df['career_level'].map(str) + ' ' + df['job_description'].map(str)\n",
    "df5=pd.DataFrame(columns=['Vacancy_id', 'bag_of_words'])\n",
    "df5['bag_of_words'] = temp.values\n",
    "df5['bag_of_words'] = df5['bag_of_words'].str.lower()\n",
    "df5['Vacancy_id']= vacancy['Vacancy_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating and generating the count matrix\n",
    "count = CountVectorizer()\n",
    "count_matrix = count.fit_transform(df5['bag_of_words'])\n",
    "\n",
    "# generating the cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(count_matrix, count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# creating a Series for the movie titles so they are associated to an ordered numerical\n",
    "# list I will use in the function to match the indexes\n",
    "indices = pd.Series(df5.Vacancy_id)\n",
    "\n",
    "#  defining the function that takes in movie title \n",
    "# as input and returns the top 10 recommended movies\n",
    "def recommendations(title, cosine_sim = cosine_sim):\n",
    "    \n",
    "    # initializing the empty list of recommended movies\n",
    "    recommended_movies = []\n",
    "    \n",
    "    # gettin the index of the movie that matches the title\n",
    "    idx = indices[indices == title].index[0]\n",
    "\n",
    "    # creating a Series with the similarity scores in descending order\n",
    "    score_series = pd.Series(cosine_sim[idx]).sort_values(ascending = False)\n",
    "\n",
    "    # getting the indexes of the 10 most similar movies\n",
    "    top_10_indexes = list(score_series.iloc[1:11].index)\n",
    "    \n",
    "    # populating the list with the titles of the best 10 matching movies\n",
    "    for i in top_10_indexes:\n",
    "        recommended_movies.append(list(df5.Vacancy_id)[i])\n",
    "        \n",
    "    return recommended_movies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_insert_values=[]\n",
    "for x in range(0, len(has_viewed)):\n",
    "    r=recommendations(has_viewed.iloc[x]['Vacancy_id3'])\n",
    "    for i in r: \n",
    "        recommended_insert_values.append((i,int(has_viewed.iloc[x]['User_id3']),0))\n",
    "#     print has_viewed.iloc[i]['Vacancy_id3'], c\n",
    "\n",
    "\n",
    "connectionObject1   = pymysql.connect(host=dbServerName, user=dbUser, password=dbPassword,db=dbName)\n",
    "try:\n",
    "    # Create a cursor object\n",
    "    cursorObject1 = connectionObject1.cursor() \n",
    "    insertStatement=\"INSERT INTO Recommended (Vacancy_id4,user_id4,sent_email) VALUES (%s,%s,%s)\"\n",
    "    cursorObject1.executemany(insertStatement,recommended_insert_values)\n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "    print(\"Exeception occured:{}\".format(e))\n",
    "\n",
    "finally:\n",
    "\n",
    "    connectionObject1.commit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
